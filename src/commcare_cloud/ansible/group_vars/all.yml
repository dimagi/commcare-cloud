cchq_home: "/home/{{ cchq_user }}"
www_dir: "{{ cchq_home }}/www"
www_home: "{{ www_dir }}/{{ deploy_env }}"
code_home: "{{ www_home }}/current"
log_home: "{{ www_home }}/log"
metrics_home: "{{ www_home }}/metrics"
legacy_service_home: "{{ code_home }}/services"
service_home: "{{ www_home }}/services"
virtualenv_home: "{{ code_home }}/python_env"
nginx_static_home: "{{ code_home }}/staticfiles"
encrypted_root: "/opt/data"
root_encryption_mode: "ecryptfs"
encrypted_tmp: "/opt/tmp"
project: "commcare-hq"
commcarehq_repository: https://github.com/dimagi/commcare-hq.git

prometheus_monitoring_enabled: False

internal_network_interface: "{{ ansible_default_ipv4.interface }}"
internal_network_interface_fact: "ansible_{{ internal_network_interface }}"
internal_ipv4: "{{ hostvars[inventory_hostname][internal_network_interface_fact].ipv4 }}"

shared_dir_gid: 1500  # This GID cannot already be allocated
shared_dir_name: "shared{{ '_' ~ deploy_env if deploy_env != 'production' else '' }}"
shared_data_dir: "/opt/{{ shared_dir_name }}"
shared_mount_dir: "/mnt/{{ shared_dir_name }}"
is_monolith: '{{ groups["all"]|length == 1 }}'
shared_drive_enabled: '{{ not is_monolith }}'
transfer_payload_dir_name: "transfer_payloads"
restore_payload_dir_name: "restore_payloads"
shared_temp_dir_name: "temp"
public_site_path: /opt/commcare-hq-public/site/output
cchq_user: cchq
cchq_group: cchq
dev_group: dimagidev

etc_hosts_lines: []
etc_hosts_lines_removed: []
ssh_allow_password: False
ssh_allow_password_users: []
django_port: 9010
zookeeper_client_port: 2181
formplayer_port: 8181

couchdb_use_haproxy: False
couchdb_version: 2.3.1
couchdb2_port: 15984
couchdb2_local_port: "{{ 15986 if couchdb_version is version('3.0.0', '<') else couchdb2_port }}"
couchdb2_proxy_port: "{{ 35984 if couchdb_use_haproxy else 25984 }}"
couchdb_admins: "{
  '{{ COUCH_USERNAME }}': '{{ COUCH_PASSWORD }}',
}"
# mostly part of couchdb setup, but also needed outside of those roles by e.g. deploy_cloudwatch_logs.yml
# which needs this variable to construct its logic on all machines, not just couchdb machines.
couchdb_dir: "{{ '/usr/local/couchdb2/couchdb' if ansible_distribution_version == '18.04' else '/opt/couchdb' }}"

is_redis_cluster: '{{ "redis_cluster_master" in groups and groups["redis_cluster_master"]|length > 1 }}'
redis_bind: 0.0.0.0

# commcare-hq connects to an S3-compatible service
s3_blob_db_enabled: False
s3_blob_db_url:
s3_blob_db_s3_bucket:
s3_bulk_delete_chunksize: 1000

# For instructions on S3-to-S3 migrations, see docs/howto/migrate-s3-to-s3.md
# or https://commcare-cloud.readthedocs.io/en/latest/services/blobdb/migrate-s3-to-s3.html
old_s3_blob_db_url: ~
old_s3_blob_db_s3_bucket: ~
old_s3_bulk_delete_chunksize: ~

# To use these vars without deploying the riak control machine
# add s3_access_key and s3_secret_key to the ansible secret config directory
s3_blob_db_access_key: "{{ s3_access_key|default('') }}"
s3_blob_db_secret_key:  "{{ s3_secret_key|default('') }}"

datadog_integration_couch: false
datadog_integration_elastic: false
datadog_integration_gunicorn: false
datadog_integration_kafka: false
datadog_integration_kafka_consumer: false
datadog_integration_nginx: false
datadog_integration_pgbouncer: false
datadog_integration_postgres: false
datadog_integration_postgres_receiver: false
datadog_integration_rabbitmq: false
datadog_integration_redisdb: false
datadog_integration_zk: false
datadog_integration_http: false
datadog_integration_haproxy: false
datadog_integration_tcp_check: false
datadog_integration_disk_check: true
datadog_extra_host_checks: []
datadog_patch_modules:
  - requests:true
  - gevent:true
  - psycopg:true
  - redis:true
  - sqlalchemy:true
  - elasticsearch:true

root_email: commcarehq-ops+root@example.com
server_email: commcarehq-noreply@example.com
server_admin_email: commcarehq-ops+admins@example.com
default_from_email: commcarehq-noreply@example.com
support_email: support@example.com
probono_support_email: pro-bono@example.com
accounts_email: accounts@example.com
data_email: datatree@example.com
subscription_change_email: accounts+subchange@example.com
internal_subscription_change_email: accounts+subchange+internal@example.com
billing_email: sales@example.com
invoicing_contact_email: accounts@example.com
growth_email: saas-revenue-team@example.com
saas_ops_email: saas-ops@example.com
saas_reporting_email: null
master_list_email: master-list@example.com
sales_email: sales@example.com
privacy_email: privacy@dimagi.com
feedback_email: hq-feedback@dimagi.com
eula_change_email: eula-notifications@example.com
contact_email: info@example.com
soft_assert_email: commcarehq-ops+soft_asserts@example.com
daily_deploy_email: null
check_s3_backups_email: null
return_path_email: null
new_domain_email: inquiries@example.com
solutions_aes_email: null
show_maintenance_updates_on_deploy: true

ALTERNATE_HOSTS: []
AMQP_HOST: "{{ groups.rabbitmq.0 }}"
AMQP_NAME: commcarehq
OLD_AMQP_HOST: null
OLD_AMQP_NAME: commcarehq
BROKER_URL: 'amqp://{{ AMQP_USER }}:{{ AMQP_PASSWORD }}@{{ AMQP_HOST }}:5672/{{ AMQP_NAME }}'
OLD_BROKER_URL: "{{ ('amqp://' + OLD_AMQP_USER + ':' + OLD_AMQP_PASSWORD + '@' + OLD_AMQP_HOST + ':5672/' + OLD_AMQP_NAME) if OLD_AMQP_HOST else None }}"
DROPBOX_APP_NAME: 'CommCareHQFiles'
ELASTICSEARCH_PORT: 9200
TWO_FACTOR_GATEWAY_ENABLED: False
internal_domain_name: null
# To enable Local ES snapshot override with repository location
es_local_repo: false
blobdb_snapshot_bucket: dimagi-{{ deploy_env }}-blobdb-backups
couchdb_snapshot_bucket: dimagi-{{ deploy_env }}-couch-backups
postgres_snapshot_bucket: dimagi-{{ deploy_env }}-postgres-backups
backup_es_s3: False
es_snapshot_bucket: "dimagi-{{ deploy_env }}-es-snapshots"
aws_region: None
# this reads "'s3.{{ aws_region }}.amazonaws.com' if aws_region else None"
aws_endpoint: '{{ aws_region and "s3." + aws_region + ".amazonaws.com" }}'
aws_versioning_enabled: true

nofile_limit: 4096
keepalived_recepient_email: '{{ root_email }}'

# this parameter is to set backups hour frequency, by default it will take backups for every '3' hours 
# if hourly backups are enabled. 
nadir_hour_frequency: 3

# celery variables to soft/hard kill tasks running for long hours.
# To enable cron job to hard/soft kill celery processes set these thresholds. Use null/~ to disable cron job.
celery_hours_before_hard_kill: ~
celery_hours_before_soft_kill: ~

ansible_become_password: "{{ ansible_sudo_pass }}"

# Java version for Formplayer
java_17_home_path: /usr/lib/jvm/java-1.17.0-openjdk-amd64
java_17_bin_path: '{{ java_17_home_path }}/bin'

#  To find this complete list, you can run:
#  $ find . -type f | grep templates/supervisor_
#  ./src/commcare_cloud/ansible/roles/commcarehq/templates/supervisor_django.conf.j2
#  ./src/commcare_cloud/ansible/roles/commcarehq/templates/supervisor_celery_beat.conf.j2
#  ./src/commcare_cloud/ansible/roles/commcarehq/templates/supervisor_prometheus.conf.j2
#  ./src/commcare_cloud/ansible/roles/commcarehq/templates/supervisor_pillowtop.conf.j2
#  ./src/commcare_cloud/ansible/roles/commcarehq/templates/supervisor_celery_workers.conf.j2
#  ./src/commcare_cloud/ansible/roles/commcarehq/templates/supervisor_formplayer_spring.conf.j2
#  ./src/commcare_cloud/ansible/roles/commcarehq/templates/supervisor_management_commands.conf.j2
#  ./src/commcare_cloud/ansible/roles/commcarehq/templates/supervisor_celery_flower.conf.j2
#
# The only ones not matching this pattern are celery_bash_runner and django_bash_runner
supervisor_service_files:
  django:
    file_path: "{{ service_home }}/{{ deploy_env }}_supervisor_django.conf"
    template: ../templates/supervisor_django.conf.j2
    should_exist: "{{ inventory_hostname in groups['webworkers'] }}"
  celery_beat:
    file_path: "{{ service_home }}/{{ deploy_env }}_supervisor_celery_beat.conf"
    template: ../templates/supervisor_celery_beat.conf.j2
    should_exist: "{{ app_processes_config.celery_processes.get(inventory_hostname).beat is defined }}"
  celery_flower:
    file_path: "{{ service_home }}/{{ deploy_env }}_supervisor_celery_flower.conf"
    template: ../templates/supervisor_celery_flower.conf.j2
    should_exist: "{{ app_processes_config.celery_processes.get(inventory_hostname).flower is defined }}"
  celery_workers:
    file_path: "{{ service_home }}/{{ deploy_env }}_supervisor_celery_workers.conf"
    template: ../templates/supervisor_celery_workers.conf.j2
    should_exist: "{{ app_processes_config.celery_processes.get(inventory_hostname) is defined }}"
  celery_bash_runner:
    file_path: "{{ service_home }}/{{ deploy_env }}_celery_bash_runner.sh"
    template: ../templates/celery_bash_runner.sh.j2
    should_exist: "{{ app_processes_config.celery_processes.get(inventory_hostname) is defined }}"
  formplayer_spring:
    file_path: "{{ service_home }}/{{ deploy_env }}_supervisor_formplayer_spring.conf"
    template: ../templates/supervisor_formplayer_spring.conf.j2
    should_exist: "{{ inventory_hostname in groups['formplayer'] }}"
  management_commands:
    file_path: "{{ service_home }}/{{ deploy_env }}_supervisor_management_commands.conf"
    template: ../templates/supervisor_management_commands.conf.j2
    should_exist: "{{ app_processes_config.management_commands.get(inventory_hostname, {}) }}"
  pillowtop:
    file_path: "{{ service_home }}/{{ deploy_env }}_supervisor_pillowtop.conf"
    template: ../templates/supervisor_pillowtop.conf.j2
    should_exist: "{{ inventory_hostname in groups['pillowtop'] }}"
  prometheus:
    file_path: "{{ service_home }}/{{ deploy_env }}_supervisor_prometheus.conf"
    template: ../templates/supervisor_prometheus.conf.j2
    should_exist: "{{ prometheus_monitoring_enabled|default(False) }}"
  # only used with prometheus
  django_bash_runner:
    file_path: "{{ service_home }}/{{ deploy_env }}_django_bash_runner.sh"
    template: ../templates/django_bash_runner.sh.j2
    should_exist: "{{ prometheus_monitoring_enabled|default(False) }}"
