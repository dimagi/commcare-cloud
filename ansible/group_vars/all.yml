cchq_home: "/home/{{ cchq_user }}"
www_dir: "{{ cchq_home }}/www"
www_home: "{{ www_dir }}/{{ deploy_env }}"
code_home: "{{ www_home }}/current"
log_home: "{{ www_home }}/log"
service_home: "{{ code_home }}/services"
virtualenv_home: "{{ code_home }}/python_env"
encrypted_root: "/opt/data"

shared_drive_enabled: true  # overridden in ../config/$ENV/$ENV.yml
shared_dir_gid: 1500  # This GID cannot already be allocated
shared_dir_name: "shared{{ '_' ~ deploy_env if deploy_env != 'production' else '' }}"
shared_data_dir: "/opt/{{ shared_dir_name }}"
shared_mount_dir: "/mnt/{{ shared_dir_name }}"
is_monolith: '{{ groups["all"]|length == 1 }}'
transfer_payload_dir_name: "transfer_payloads"
restore_payload_dir_name: "restore_payloads"
shared_temp_dir_name: "temp"
public_site_path: /opt/commcare-hq-public/site/output
commtrack_public_site_path: /opt/commtrack-static/site/output/
cchq_user: cchq
dev_group: dimagidev
dev_users:
  present:
    - astone
    - biyeun
    - cellowitz
    - ctsims
    - czue
    - dmiller
    - droberts
    - esoergel
    - frener
    - gbova
    - gcapalbo
    - jemord
    - jroth
    - jschweers
    - jwilson
    - mkangia
    - npellegrino
    - nhooper
    - nsharma
    - pvaidyanathan
    - sgoyal
    - skelly
    - sravfeyn
    - wpride
    - cchq
  absent:
    - preseed
    - twymer
    - tsheffels
    - dmyung
    - jenkins_slave
    - brudolph
    - ncarnahan
    - jhoffman

ssh_allow_password: False
django_port: 9010
riakcs_port: 8080
riakcs_proxy_port_override: "{{ riakcs_port }}"  # use this to override in env vars
riakcs_proxy_port: "{{ riakcs_proxy_port_override }}"
temp_riakcs_proxy_port: 8081
riakcs_control: "{{ groups.get('riakcs')[0] | default('riakcs-is-disabled') }}"
zookeeper_client_port: 2181
formplayer_port: 8181
couchdb2_port: 15984
couchdb2_proxy_port: 25984

# commcare-hq connects to an S3-compatible service, which is Riak CS
s3_blob_db_enabled: "{{ 'true' if groups.get('riakcs') else '' }}"
s3_blob_db_url: "http://{{ groups['proxy'][0] }}:{{ temp_riakcs_proxy_port if 'BLOB_DB_MIGRATING_FROM_S3_TO_S3' in localsettings else riakcs_proxy_port }}"

# Process for migrating from old to new riak cluster:
#
# - add [riakcs_new] hosts (of new riak cluster) to inventory
# - add `localsettings.BLOB_DB_MIGRATING_FROM_S3_TO_S3: True` in environments/$ENV/public.yml
# - add/update settings in environments/$ENV/vault.yml
#   - secrets.RIAKCS_S3_ACCESS_KEY        # new cluster
#   - secrets.RIAKCS_S3_SECRET_KEY        # new cluster
#   - secrets.OLD_S3_BLOB_DB_ACCESS_KEY   # old cluster
#   - secrets.OLD_S3_BLOB_DB_SECRET_KEY   # old cluster
# - deploy proxy (old=8080, new=8081)
# - deploy localsettings
#   during this deploy hosts with old localsettings will continue to talk
#   to old riak cluster on port 8080
#
# - run migration
#
# - move [riakcs_new] hosts to [riakcs] (and delete old hosts) in inventory
# - update environments/$ENV/public.yml
#   - remove `localsettings.BLOB_DB_MIGRATING_FROM_S3_TO_S3: True`
#   - add    `localsettings.TEMP_RIAKCS_PROXY: True`
# - deploy proxy (new=8080, new=8081)
# - deploy localsettings
#   during this deploy hosts with old localsettings will continue to talk
#   to new riak cluster on port 8081, and once updated will talk to new riak
#   cluster proxied on port 8080. There is a slight chance of MigratingBlobDB
#   failover from new to new, but this should be rare and benign.
#
# - remove `localsettings.TEMP_RIAKCS_PROXY: True` from environments/$ENV/public.yml
# - deploy proxy (new=8080)
old_s3_blob_db_url: "http://{{ groups['proxy'][0] }}:{{ riakcs_proxy_port }}"

# To use these vars without deploying the riak control machine
# add riakcs_s3_access_key and riakcs_s3_secret_key to the ansible secret config directory
s3_blob_db_access_key: "{{ riakcs_s3_access_key if riakcs_s3_access_key is defined else (hostvars[riakcs_control]['riak_key'] if s3_blob_db_enabled else '') }}"
s3_blob_db_secret_key:  "{{ riakcs_s3_secret_key if riakcs_s3_secret_key is defined else (hostvars[riakcs_control]['riak_secret'] if s3_blob_db_enabled else '') }}"

datadog_integration_cloudant: false
datadog_integration_couch: false
datadog_integration_elastic: false
datadog_integration_gunicorn: false
datadog_integration_kafka: false
datadog_integration_kafka_consumer: false
datadog_integration_nginx: false
datadog_integration_pgbouncer: false
datadog_integration_postgres: false
datadog_integration_rabbitmq: false
datadog_integration_redisdb: false
datadog_integration_riak: false
datadog_integration_riakcs: false
datadog_integration_zk: false
datadog_integration_jmx: false
datadog_integration_vmware: false

root_email: commcarehq-ops+root@dimagi.com
server_email: commcarehq-noreply@dimagi.com
default_from_email: commcarehq-noreply@dimagi.com
support_email: support@dimagi.com
probono_support_email: pro-bono@dimagi.com
cchq_bug_report_email: commcarehq-bug-reports@dimagi.com
accounts_email: accounts@dimagi.com
data_email: datatree@dimagi.com
subscription_change_email: accounts+subchange@dimagi.com
internal_subscription_change_email: accounts+subchange+internal@dimagi.com
billing_email: billing-comm@dimagi.com
invoicing_contact_email: billing-support@dimagi.com
growth_email: growth@dimagi.com
master_list_email: master-list@dimagi.com
report_builder_add_on_email: sales@dimagi.com
eula_change_email: eula-notifications@dimagi.com
contact_email: info@dimagi.com
soft_assert_email: commcarehq-ops+soft_asserts@dimagi.com
daily_deploy_email: null


AMQP_PASSWORD: "{{ secrets.AMQP_PASSWORD | default(None) }}"
AMQP_USER: "{{ secrets.AMQP_USER | default(None) }}"
DATADOG_API_KEY: "{{ secrets.DATADOG_API_KEY | default(None) }}"
DATADOG_APP_KEY: "{{ secrets.DATADOG_APP_KEY | default(None) }}"
ECRYPTFS_PASSWORD: "{{ secrets.ECRYPTFS_PASSWORD | default(None) }}"
KSPLICE_ACTIVATION_KEY: "{{ secrets.KSPLICE_ACTIVATION_KEY | default(None) }}"
NEW_RELIC_KEY: "{{ secrets.NEW_RELIC_KEY | default(None) }}"
ansible_user_password_sha_512: "{{ secrets.ANSIBLE_USER_PASSWORD_SHA_512 | default(None) }}"
formplayer_sentry_dsn: '{{ secrets.FORMPLAYER_SENTRY_DSN | default(None) }}'
old_s3_blob_db_access_key: "{{ secrets.OLD_S3_BLOB_DB_ACCESS_KEY | default(None) }}"
old_s3_blob_db_secret_key: "{{ secrets.OLD_S3_BLOB_DB_SECRET_KEY | default(None) }}"
postgres_users: "{{ secrets.POSTGRES_USERS | default(None) }}"
riakcs_s3_access_key: "{{ secrets.RIAKCS_S3_ACCESS_KEY | default(None) }}"
riakcs_s3_secret_key: "{{ secrets.RIAKCS_S3_SECRET_KEY | default(None) }}"
supervisor_http_password: "{{ secrets.SUPERVISOR_HTTP_PASSWORD | default(None) }}"
supervisor_http_username: "{{ secrets.SUPERVISOR_HTTP_USERNAME | default(None) }}"
# To enable Local ES snapshot override with repository location
es_local_repo: false

blobdb_snapshot_bucket: dimagi-{{ deploy_env }}-blobdb-backups
couchdb_snapshot_bucket: dimagi-{{ deploy_env }}-couch-backups
postgres_snapshot_bucket: dimagi-{{ deploy_env }}-postgres-backups
es_snapshot_bucket: "dimagi-{{ deploy_env }}-es-snapshots"
aws_endpoint: "s3.{{ aws_region }}.amazonaws.com"
