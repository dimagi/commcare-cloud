cchq_home: "/home/{{ cchq_user }}"
www_dir: "{{ cchq_home }}/www"
www_home: "{{ www_dir }}/{{ deploy_env }}"
code_home: "{{ www_home }}/current"
log_home: "{{ www_home }}/log"
legacy_service_home: "{{ code_home }}/services"
service_home: "{{ www_home }}/services"
virtualenv_home: "{{ code_home }}/python_env"
encrypted_root: "/opt/data"
project: "commcare-hq"

shared_drive_enabled: true
shared_dir_gid: 1500  # This GID cannot already be allocated
shared_dir_name: "shared{{ '_' ~ deploy_env if deploy_env != 'production' else '' }}"
shared_data_dir: "/opt/{{ shared_dir_name }}"
shared_mount_dir: "/mnt/{{ shared_dir_name }}"
is_monolith: '{{ groups["all"]|length == 1 }}'
transfer_payload_dir_name: "transfer_payloads"
restore_payload_dir_name: "restore_payloads"
shared_temp_dir_name: "temp"
public_site_path: /opt/commcare-hq-public/site/output
commtrack_public_site_path: /opt/commtrack-static/site/output/
cchq_user: cchq
dev_group: dimagidev

ssh_allow_password: False
django_port: 9010
riakcs_port: 8080
riakcs_proxy_port_override: "{{ riakcs_port }}"  # use this to override in env vars
riakcs_proxy_port: "{{ riakcs_proxy_port_override }}"
temp_riakcs_proxy_port: 8081
riakcs_control: "{{ groups.get('riakcs')[0] | default('riakcs-is-disabled') }}"
zookeeper_client_port: 2181
formplayer_port: 8181
couchdb2_port: 15984
couchdb2_proxy_port: 25984

# commcare-hq connects to an S3-compatible service, which is Riak CS
s3_blob_db_enabled: "{{ 'true' if groups.get('riakcs') else '' }}"
s3_blob_db_url: "http://{{ groups['proxy'][0] }}:{{ temp_riakcs_proxy_port if 'BLOB_DB_MIGRATING_FROM_S3_TO_S3' in localsettings else riakcs_proxy_port }}"

# Process for migrating from old to new riak cluster:
#
# - add [riakcs_new] hosts (of new riak cluster) to inventory
# - add `localsettings.BLOB_DB_MIGRATING_FROM_S3_TO_S3: True` in environments/$ENV/public.yml
# - add/update settings in environments/$ENV/vault.yml
#   - secrets.RIAKCS_S3_ACCESS_KEY        # new cluster
#   - secrets.RIAKCS_S3_SECRET_KEY        # new cluster
#   - secrets.OLD_S3_BLOB_DB_ACCESS_KEY   # old cluster
#   - secrets.OLD_S3_BLOB_DB_SECRET_KEY   # old cluster
# - deploy proxy (old=8080, new=8081)
# - deploy localsettings
#   during this deploy hosts with old localsettings will continue to talk
#   to old riak cluster on port 8080
#
# - run migration
#
# - move [riakcs_new] hosts to [riakcs] (and delete old hosts) in inventory
# - update environments/$ENV/public.yml
#   - remove `localsettings.BLOB_DB_MIGRATING_FROM_S3_TO_S3: True`
#   - add    `localsettings.TEMP_RIAKCS_PROXY: True`
# - deploy proxy (new=8080, new=8081)
# - deploy localsettings
#   during this deploy hosts with old localsettings will continue to talk
#   to new riak cluster on port 8081, and once updated will talk to new riak
#   cluster proxied on port 8080. There is a slight chance of MigratingBlobDB
#   failover from new to new, but this should be rare and benign.
#
# - remove `localsettings.TEMP_RIAKCS_PROXY: True` from environments/$ENV/public.yml
# - deploy proxy (new=8080)
old_s3_blob_db_url: "http://{{ groups['proxy'][0] }}:{{ riakcs_proxy_port }}"

# To use these vars without deploying the riak control machine
# add riakcs_s3_access_key and riakcs_s3_secret_key to the ansible secret config directory
s3_blob_db_access_key: "{{ riakcs_s3_access_key if riakcs_s3_access_key else (hostvars[riakcs_control]['riak_key'] if s3_blob_db_enabled else '') }}"
s3_blob_db_secret_key:  "{{ riakcs_s3_secret_key if riakcs_s3_secret_key else (hostvars[riakcs_control]['riak_secret'] if s3_blob_db_enabled else '') }}"

couchdb_mirror: https://archive.apache.org/dist/couchdb/source

datadog_integration_cloudant: false
datadog_integration_couch: false
datadog_integration_elastic: false
datadog_integration_gunicorn: false
datadog_integration_kafka: false
datadog_integration_kafka_consumer: false
datadog_integration_nginx: false
datadog_integration_pgbouncer: false
datadog_integration_postgres: false
datadog_integration_rabbitmq: false
datadog_integration_redisdb: false
datadog_integration_riak: false
datadog_integration_riakcs: false
datadog_integration_zk: false
datadog_integration_jmx: false
datadog_integration_vmware: false

root_email: commcarehq-ops+root@dimagi.com
server_email: commcarehq-noreply@dimagi.com
default_from_email: commcarehq-noreply@dimagi.com
support_email: support@dimagi.com
probono_support_email: pro-bono@dimagi.com
cchq_bug_report_email: commcarehq-bug-reports@dimagi.com
accounts_email: accounts@dimagi.com
data_email: datatree@dimagi.com
subscription_change_email: accounts+subchange@dimagi.com
internal_subscription_change_email: accounts+subchange+internal@dimagi.com
billing_email: billing-comm@dimagi.com
invoicing_contact_email: billing-support@dimagi.com
growth_email: growth@dimagi.com
master_list_email: master-list@dimagi.com
report_builder_add_on_email: sales@dimagi.com
eula_change_email: eula-notifications@dimagi.com
contact_email: info@dimagi.com
soft_assert_email: commcarehq-ops+soft_asserts@dimagi.com
daily_deploy_email: null


AMQP_PASSWORD: "{{ secrets.AMQP_PASSWORD | default(None) }}"
AMQP_USER: "{{ secrets.AMQP_USER | default(None) }}"
DATADOG_API_KEY: "{{ secrets.DATADOG_API_KEY | default(None) }}"
DATADOG_APP_KEY: "{{ secrets.DATADOG_APP_KEY | default(None) }}"
ECRYPTFS_PASSWORD: "{{ secrets.ECRYPTFS_PASSWORD | default(None) }}"
KSPLICE_ACTIVATION_KEY: "{{ secrets.KSPLICE_ACTIVATION_KEY | default(None) }}"
NEW_RELIC_KEY: "{{ secrets.NEW_RELIC_KEY | default(None) }}"
ansible_user_password_sha_512: "{{ secrets.ANSIBLE_USER_PASSWORD_SHA_512 | default(None) }}"
formplayer_sentry_dsn: '{{ secrets.FORMPLAYER_SENTRY_DSN | default(None) }}'
old_s3_blob_db_access_key: "{{ secrets.OLD_S3_BLOB_DB_ACCESS_KEY | default(None) }}"
old_s3_blob_db_secret_key: "{{ secrets.OLD_S3_BLOB_DB_SECRET_KEY | default(None) }}"
postgres_users: "{{ secrets.POSTGRES_USERS | default(None) }}"
riakcs_s3_access_key: "{{ secrets.RIAKCS_S3_ACCESS_KEY | default(None) }}"
riakcs_s3_secret_key: "{{ secrets.RIAKCS_S3_SECRET_KEY | default(None) }}"
supervisor_http_password: "{{ secrets.SUPERVISOR_HTTP_PASSWORD | default(None) }}"
supervisor_http_username: "{{ secrets.SUPERVISOR_HTTP_USERNAME | default(None) }}"
# To enable Local ES snapshot override with repository location
es_local_repo: false
blobdb_snapshot_bucket: dimagi-{{ deploy_env }}-blobdb-backups
couchdb_snapshot_bucket: dimagi-{{ deploy_env }}-couch-backups
postgres_snapshot_bucket: dimagi-{{ deploy_env }}-postgres-backups
es_snapshot_bucket: "dimagi-{{ deploy_env }}-es-snapshots"
aws_region: None
# this reads "'s3.{{ aws_region }}.amazonaws.com' if aws_region else None"
aws_endpoint: '{{ aws_region and "s3." + aws_region + ".amazonaws.com" }}'
aws_versioning_enabled: true
